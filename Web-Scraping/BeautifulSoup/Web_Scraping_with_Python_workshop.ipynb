{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<h1> Web Scraping with Python </h1>"
      ],
      "metadata": {
        "id": "1PQu0RyLcitR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Web scraping is an automatic method to obtain large amounts of data from websites.\n",
        "Web scraping is used for:\n",
        "- Price monitoring\n",
        "- Market research\n",
        "- News monitoring\n",
        "- Sentiment analysis\n",
        "- Email marketing\n",
        "- Collecting data for machine learning and deep learning models\n",
        "\n",
        "Web pages can either be scraped using APIs or parsing web pages.\n",
        "There are various libraries for parsing web pages using Python. These include BeautifulSoup, Scrapy, Selenium, ZenRows, Playwright etc."
      ],
      "metadata": {
        "id": "faTN-js5dQr0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Legal and Ethical considerations for web scraping\n",
        "Just because data is present on the internet, it doesn't mean it can or should be scraped.\n",
        "Always look into permissions, what data can be scraped, how much can be scraped and how are you allowed to use this data.\n",
        "A robots.txt file specifies which pages of a website can and can't be accessed for crawling and scraping."
      ],
      "metadata": {
        "id": "7lxVjoJ7doZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Regular Expressions"
      ],
      "metadata": {
        "id": "k2JFCqDclM9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A regular expression (RegEx) is a sequence of characters that forms a search pattern. This is used to check if a string has a specified pattern.\n",
        "Regular expressions are used to select a specific pattern of information in web scraping.\n",
        "\n",
        "Function | Description\n",
        "---------|------------\n",
        "findall (match_pattern, search_string) | Returns a list containing all matches\n",
        "search (match_pattern, search_string) | Returns a Match object if there is a match anywhere in the string (only first occurrence)\n",
        "split (match_pattern, search_string, maxsplit) | Returns a list where the string has been split at each match\n",
        "sub (match_pattern, replacement, search_string, count) | Replaces one or many matches with a string\n"
      ],
      "metadata": {
        "id": "SFO0-Sl1lgeV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "string = \"Stop the spinning top.\""
      ],
      "metadata": {
        "id": "4VfsSt8rlf-R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Match Object**\n",
        "\n",
        "The Match object has information about the search and result.\n",
        "- .span() returns a tuple with the start and end position of the match.\n",
        "- .string returns the string passed to the search function.\n",
        "- .group() returns the part of the string where the match was found\n",
        "\n"
      ],
      "metadata": {
        "id": "7DszZSWG96gS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "string = \"Stop the spinning top.\"\n",
        "\n",
        "x = re.search (r'spin', string)\n",
        "print(x)    # x = None if the pattern is not found\n",
        "print(\"span:\", x.span())\n",
        "print(\"string:\", x.string)\n",
        "print(\"group:\", x.group())"
      ],
      "metadata": {
        "id": "Ykk3_MKD9oRh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Symbol | Description |\n",
        "|---|---|\n",
        "| ^ | Matches at the beginning of a line |\n",
        "| $ | Matches at the end of the line |\n",
        "| . | Matches any character |\n",
        "| \\s | Matches whitespace |\n",
        "| \\S | Matches any non-whitespace character |\n",
        "| * | Zero or more occurrences |\n",
        "| + | One or more occurrences |\n",
        "| ? | Zero or one occurrences |\n",
        "| [aeiou] | Matches a single character in the listed set |\n",
        "| [^XYZ] | Matches a single character not in the listed set |\n",
        "| [a-z0-9] | The set of characters can include a range |\n",
        "| ( | Indicates where string extraction is to start |\n",
        "| ) | Indicates where string extraction is to end |\n",
        "| \\b | Returns a match where the specified characters are at the beginning or at the end of a word |\n",
        "| \\B | Returns a match where the specified characters are present, but NOT at the beginning or at the end of a word |"
      ],
      "metadata": {
        "id": "GiSLGmAM0L7S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "txt1 = 'the cat sits on the cot.'"
      ],
      "metadata": {
        "id": "aX5q_Ctn1JbU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# words starting with c and ending with t"
      ],
      "metadata": {
        "id": "poS86kvE3Prm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# find all 'the's"
      ],
      "metadata": {
        "id": "_od-59fH3T3B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# find starting 'the's"
      ],
      "metadata": {
        "id": "R35FP2k-3ePR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extract words"
      ],
      "metadata": {
        "id": "e5FLc2Nx3idU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "txt2 = 'THE CAT EATS A CARROT IN THE CT SCAN.'\n",
        "\n",
        "ex5 = re.findall('C.?T', txt2)\n",
        "print('C.?T:', ex5)"
      ],
      "metadata": {
        "id": "DDeTN_oR4Vkp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "outputId": "aa94cd9e-9ea9-444a-8601-77b1acb7708e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 're' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-a87229bb683c>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtxt2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'THE CAT EATS A CARROT IN THE CT SCAN.'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mex5\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'C.?T'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtxt2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'C.?T:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mex5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 're' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "txt3 = \"\"\"The majestic mountains towered above the quaint little village nestled in the valley, while the river meandered\n",
        "gently through the lush greenery, creating a serene and picturesque landscape.\"\"\"\n",
        "\n",
        "vowels = re.findall('[aeiouAEIOU]', txt3)\n",
        "print(len(vowels))\n",
        "consonants = re.findall('[^aeiouAEIOU0-9\\s\\W_]', txt3)\n",
        "print(len(consonants))"
      ],
      "metadata": {
        "id": "8Js1_5vf6U0G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## HTTP Libraries for Python\n",
        "In order to scrape data from websites, first a connection must be established with the website."
      ],
      "metadata": {
        "id": "ic1U4zBy7aFF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**urllib** <br>\n",
        "urllib is a package that is part of the Python Standard Library. It has several modules for working with URLs like:\n",
        "- urllib.request for opening and reading URLs\n",
        "- urllib.error containing the exceptions raised by urllib.request\n",
        "- urllib.parse for parsing URLs\n",
        "- urllib.robotparser for parsing robots.txt files <br><br>\n",
        "It is suitable for simple tasks but lacks many features that are required for more complex web interactions."
      ],
      "metadata": {
        "id": "33le--vl7iuA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**urllib3** <br>\n",
        "urllib3 is a powerful, user-friendly third-party HTTP client for Python, which includes additional features like:\n",
        "- Thread safety.\n",
        "- Connection pooling.\n",
        "- Client-side TLS/SSL verification.\n",
        "- File uploads with multipart encoding.\n",
        "- Helpers for retrying requests and dealing with HTTP redirects.\n",
        "- Support for gzip, deflate, brotli, and zstd encoding.\n",
        "- Proxy support for HTTP and SOCKS.\n",
        "- 100% test coverage."
      ],
      "metadata": {
        "id": "9smDGCzi8Lom"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Requests** <br>\n",
        "Requests is a simple and elegant HTTP library for Python, built on top of urllib3."
      ],
      "metadata": {
        "id": "0vasueIJ8cKj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Understanding HTTP Headers\n",
        "A HTTP header gives additional information (metadata) about the request or response. <br>\n",
        "There are 3 types of headers: General, Response and Request headers."
      ],
      "metadata": {
        "id": "_37EQzae8uiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Web Scraping with API\n"
      ],
      "metadata": {
        "id": "9eaxaqUPb_cF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MA66kp2sXap0"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://openlibrary.org/search/authors.json?q=twain\"\n",
        "response = requests.get(url)"
      ],
      "metadata": {
        "id": "lEPhr9tcXoG0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View response headers\n",
        "print(\"Response Headers:\")\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "# View request headers\n",
        "print(\"Request Headers:\")"
      ],
      "metadata": {
        "id": "Czs_ujtxYDlb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View General Headers\n",
        "\n",
        "# Accessing Request URL\n",
        "\n",
        "\n",
        "# Accessing Request Method\n",
        "\n",
        "\n",
        "# Accessing Status Code\n",
        "\n"
      ],
      "metadata": {
        "id": "DIN5yQSiYhSx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib import robotparser"
      ],
      "metadata": {
        "id": "igCH_d_3BcT2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_info(book_name)"
      ],
      "metadata": {
        "id": "GZJFxgteZIdB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "book1 = input('Enter 1st book name: ')\n",
        "book2 = input('Enter 2nd book name: ')"
      ],
      "metadata": {
        "id": "z0BtMs2UZows"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sub1, chars1 = get_info(book1)\n",
        "sub1 = set(sub1)\n",
        "chars1 = set(chars1)\n",
        "sub2, chars2 = get_info(book2)\n",
        "sub2 = set(sub2)\n",
        "chars2 = set(chars2)\n",
        "\n",
        "common_sub = list(sub1 & sub2)\n",
        "print('Common subjects are:',common_sub)\n",
        "common_chars = list(chars1 & chars2)\n",
        "print('Common characters are:',common_chars)"
      ],
      "metadata": {
        "id": "XCYnrp2HZy4Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Web Scraping with *BeautifulSoup*"
      ],
      "metadata": {
        "id": "7K2Z02bpcI9S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request, urllib.parse\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "WBpY2PsPcUpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape_books(url)"
      ],
      "metadata": {
        "id": "3XCxf8-IAGuD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_url = 'https://books.toscrape.com/catalogue/category/books/historical-fiction_4/index.html'\n",
        "rp_webs = robotparser.RobotFileParser()\n",
        "rp_webs.set_url(f\"{start_url}/robots.txt\")\n",
        "rp_webs.read()\n",
        "print(rp_webs.can_fetch('*', start_url))"
      ],
      "metadata": {
        "id": "TILM9FDTAGsa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Response Headers"
      ],
      "metadata": {
        "id": "liyw_Gj1KaCu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Some General Headers"
      ],
      "metadata": {
        "id": "oQYrAW4AKbrC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "titles, ratings, prices = scrape_books(start_url)"
      ],
      "metadata": {
        "id": "7ysTxFBWCVoT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = {'Title': titles, 'Rating': ratings, 'Price': prices}\n",
        "df = pd.DataFrame(data)"
      ],
      "metadata": {
        "id": "yiBV8VBsANZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Price'] = df['Price'].str.replace('£', '')\n",
        "print(df)"
      ],
      "metadata": {
        "id": "Y7935q9-AbUu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('books_data.csv', index=False)"
      ],
      "metadata": {
        "id": "9FjxxLWGAjFi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}